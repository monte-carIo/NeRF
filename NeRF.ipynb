{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test for Lagrange multiplier\n",
        "import numpy as np\n",
        "x = np.array([1,2,3])\n",
        "y = np.array([4,5,6])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2fo27VhBlnr"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptTYjWao3VsM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Optional, Tuple, List, Union, Callable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "from tqdm import trange\n",
        "\n",
        "# For repeatability\n",
        "# seed = 3407\n",
        "# torch.manual_seed(seed)\n",
        "# np.random.seed(seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBZ59MyICMuM"
      },
      "source": [
        "# Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toIYVxPL5IDO"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNLc9hbd6JGK"
      },
      "source": [
        "First we load the data which we will train our NeRF model on. This is the Lego bulldozer commonly seen in the NeRF demonstrations and serves as a sort of \"Hello World\" for training NeRFs. Covering other datasets is outside the scope of this notebook, but feel free to try others included in the original [NeRF source code](https://github.com/bmild/nerf) or your own datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVAGjdUthecA",
        "outputId": "4d1bec8c-5ca5-4f1d-bcd3-cc5772e6a591"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('tiny_nerf_data.npz'):\n",
        "  !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHpKDWmS65-w"
      },
      "source": [
        "This dataset consists of 106 images taken of the synthetic Lego bulldozer along with poses and a common focal length value. Like the original, we reserve the first 100 images for training and a single test image for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "csCQJhSzhnEW",
        "outputId": "d8cc0cf8-f088-440a-c944-38cba3873e3c"
      },
      "outputs": [],
      "source": [
        "data = np.load('tiny_nerf_data.npz')\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "\n",
        "print(f'Images shape: {images.shape}')\n",
        "print(f'Poses shape: {poses.shape}')\n",
        "print(f'Focal length: {focal}')\n",
        "\n",
        "height, width = images.shape[1:3]\n",
        "near, far = 2., 6.\n",
        "\n",
        "n_training = 100\n",
        "testimg_idx = 101\n",
        "testimg, testpose = images[testimg_idx], poses[testimg_idx]\n",
        "\n",
        "plt.imshow(testimg)\n",
        "print('Pose')\n",
        "print(testpose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa8rlY5djYvt"
      },
      "source": [
        "## Origins and Directions\n",
        "\n",
        "Recall that NeRF processes inputs from a field of positions (x,y,z) and view directions (θ,φ). To gather these input points, we need to apply inverse rendering to the input images. More concretely, we draw projection lines through each pixel and across the 3D space, from which we can draw samples.\n",
        "\n",
        "To sample points from the 3D space beyond our image, we first start from the initial pose of every camera taken in the photo set. With some vector math, we can convert these 4x4 pose matrices into a 3D coordinate denoting the origin and a 3D vector indicating the direction. The two together describe a vector that indicates where a camera was pointing when the photo was taken.\n",
        "\n",
        "The code in the cell below illustrates this by drawing arrows that depict the origin and the direction of every frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Wvq5Y663jYHE",
        "outputId": "645886f4-c937-4861-9548-8b6636d81cc6"
      },
      "outputs": [],
      "source": [
        "dirs = np.stack([np.sum([0, 0, -1] * pose[:3, :3], axis=-1) for pose in poses])\n",
        "# get the origin of the camera frame\n",
        "origins = poses[:, :3, -1]\n",
        "\n",
        "# Plot the camera directions\n",
        "ax = plt.figure(figsize=(12, 8)).add_subplot(projection='3d')\n",
        "_ = ax.quiver(\n",
        "  origins[..., 0],\n",
        "  origins[..., 1],\n",
        "  origins[..., 2],\n",
        "  dirs[..., 0],\n",
        "  dirs[..., 1],\n",
        "  dirs[..., 2], length=0.5, normalize=True)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('z')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUI5BA9BXlTm"
      },
      "source": [
        "With this camera pose, we can now find the projection lines along each pixel of our image. Each line is defined by its origin point (x,y,z) and its direction (in this case a 3D vector). While the origin is the same for every pixel, the direction is slightly different. These lines are slightly deflected off center such that none of these lines are parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHNwlsOT7NTp"
      },
      "outputs": [],
      "source": [
        "def get_rays(\n",
        "  height: int,\n",
        "  width: int,\n",
        "  focal_length: float,\n",
        "  c2w: torch.Tensor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Find origin and direction of rays through every pixel and camera origin.\n",
        "  \"\"\"\n",
        "\n",
        "  # Apply pinhole camera model to gather directions at each pixel\n",
        "  i, j = torch.meshgrid(\n",
        "      torch.arange(width, dtype=torch.float32).to(c2w),\n",
        "      torch.arange(height, dtype=torch.float32).to(c2w),\n",
        "      indexing='ij')\n",
        "  i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
        "  # i and j are now of shape [height, width]\n",
        "  # i = [0, 1, 2, ..., width - 1] for every row\n",
        "  # j = [0, 1, 2, ..., height - 1] for every column\n",
        "  directions = torch.stack([(i - width * .5) / focal_length,\n",
        "                            -(j - height * .5) / focal_length,\n",
        "                            -torch.ones_like(i)\n",
        "                           ], dim=-1)\n",
        "  # directions is now of shape [height, width, 3] and contains the direction\n",
        "\n",
        "  # Apply camera pose to directions\n",
        "  rays_d = torch.sum(directions[..., None, :] * c2w[:3, :3], dim=-1)\n",
        "  \n",
        "\n",
        "  # Origin is same for all directions (the optical center)\n",
        "  rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
        "  return rays_o, rays_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i, j = torch.meshgrid(\n",
        "      torch.arange(10, dtype=torch.float32).to(device='cpu'),\n",
        "      torch.arange(10, dtype=torch.float32).to(device='cpu'),\n",
        "      indexing='ij')\n",
        "i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
        "directions = torch.stack([(i - 10 * .5) ,\n",
        "                            -(j - 10 * .5) ,\n",
        "                            -torch.ones_like(i)\n",
        "                           ], dim=-1)\n",
        "# testpose = torch.tensor([[ 1, 2, 3, 4],\n",
        "#         [5,6,7,8],\n",
        "#         [ 9,10,11,12],\n",
        "#         [ 13,14,15,16]])\n",
        "# rays_d = torch.sum(directions[..., None, :] * testpose[:3, :3], dim=-1)\n",
        "print(-(j - 10 * .5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create three tensors of the same shape\n",
        "tensor1 = torch.tensor([1, 2, 3])\n",
        "tensor2 = torch.tensor([4, 5, 6])\n",
        "tensor3 = torch.tensor([7, 8, 9])\n",
        "\n",
        "# Stack the tensors along a new dimension (default is dim=0)\n",
        "stacked_tensor = torch.stack([tensor1, tensor2, tensor3], dim=-1)\n",
        "\n",
        "# Print the stacked tensor\n",
        "print(stacked_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYPFqClGkKD3",
        "outputId": "edf14a48-94b7-4ce8-91fc-61e7152e59f1"
      },
      "outputs": [],
      "source": [
        "testpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aivi7gXLkPTP",
        "outputId": "796318a9-e03a-46de-d6eb-752dc4776d8a"
      },
      "outputs": [],
      "source": [
        "focal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoV1r440lCQB",
        "outputId": "47c99684-f9a6-4a00-fcd5-ec36ca19d21c"
      },
      "outputs": [],
      "source": [
        "# Gather as torch tensors\n",
        "images = torch.from_numpy(data['images'][:n_training]).to(device)\n",
        "poses = torch.from_numpy(data['poses']).to(device)\n",
        "focal = torch.from_numpy(data['focal']).to(device)\n",
        "testimg = torch.from_numpy(data['images'][testimg_idx]).to(device)\n",
        "testpose = torch.from_numpy(data['poses'][testimg_idx]).to(device)\n",
        "\n",
        "# Grab rays from sample image\n",
        "height, width = images.shape[1:3]\n",
        "with torch.no_grad():\n",
        "  ray_origin, ray_direction = get_rays(height, width, focal, testpose)\n",
        "\n",
        "print('Ray Origin')\n",
        "print(ray_origin.shape)\n",
        "print(ray_origin[height // 2, width // 2, :])\n",
        "print('')\n",
        "\n",
        "print('Ray Direction')\n",
        "print(ray_direction.shape)\n",
        "print(ray_direction[height // 2, width // 2, :])\n",
        "print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU1QDn66CQob"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yg3f_12FYjw"
      },
      "source": [
        "## Stratified Sampling\n",
        "\n",
        "Now that we have these lines, defined as origin and direction vectors, we can begin the process of sampling them. Recall that NeRF takes a coarse-to-fine sampling strategy, starting with the stratified sampling approach.\n",
        "\n",
        "The stratified sampling approach splits the ray into evenly-spaced bins and randomly samples within each bin. The `perturb` setting determines whether to sample points uniformly from each bin or to simply use the bin center as the point. In most cases, we want to keep `perturb = True` as it will encourage the network to learn over a continuously sampled space. It may be useful to disable for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAAfDK2L-faR"
      },
      "outputs": [],
      "source": [
        "def sample_stratified(\n",
        "  rays_o: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  near: float,\n",
        "  far: float,\n",
        "  n_samples: int,\n",
        "  perturb: Optional[bool] = True,\n",
        "  inverse_depth: bool = False\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Sample along ray from regularly-spaced bins.\n",
        "  \"\"\"\n",
        "\n",
        "  # Grab samples for space integration along ray\n",
        "  t_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
        "  if not inverse_depth:\n",
        "    # Sample linearly between `near` and `far`\n",
        "    z_vals = near * (1.-t_vals) + far * (t_vals)\n",
        "  else:\n",
        "    # Sample linearly in inverse depth (disparity)\n",
        "    z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
        "\n",
        "  # Draw uniform samples from bins along ray\n",
        "  if perturb:\n",
        "    mids = .5 * (z_vals[1:] + z_vals[:-1])\n",
        "    upper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
        "    lower = torch.concat([z_vals[:1], mids], dim=-1)\n",
        "    t_rand = torch.rand([n_samples], device=z_vals.device)\n",
        "    z_vals = lower + (upper - lower) * t_rand\n",
        "  z_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
        "\n",
        "  # Apply scale from `rays_d` and offset from `rays_o` to samples\n",
        "  # pts: (width, height, n_samples, 3)\n",
        "  pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
        "  return pts, z_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCWWyvuLm0VE",
        "outputId": "e009fe7c-c587-4479-83c5-d16f71c40aa8"
      },
      "outputs": [],
      "source": [
        "# Draw stratified samples from example\n",
        "rays_o = ray_origin.view([-1, 3])\n",
        "rays_d = ray_direction.view([-1, 3])\n",
        "n_samples = 8\n",
        "perturb = True\n",
        "inverse_depth = False\n",
        "with torch.no_grad():\n",
        "  pts, z_vals = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
        "                                  perturb=perturb, inverse_depth=inverse_depth)\n",
        "print('Input Points')\n",
        "print(pts.shape)\n",
        "print('')\n",
        "print('Distances Along Ray')\n",
        "print(z_vals.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEeLXT1pc-c2"
      },
      "source": [
        "Now we visualize these sampled points. The unperturbed blue points are the bin \"centers.\" The red points are a sampling of perturbed points. Notice how the red points are slightly offset from the blue points above them, but all are constrained between `near` and `far`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "lt3zLUginJ0B",
        "outputId": "bd172f4a-9961-4c09-b0de-32d6642ad637"
      },
      "outputs": [],
      "source": [
        "y_vals = torch.zeros_like(z_vals)\n",
        "\n",
        "_, z_vals_unperturbed = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
        "                                  perturb=False, inverse_depth=False)\n",
        "plt.plot(z_vals_unperturbed[0].cpu().numpy(), 1 + y_vals[0].cpu().numpy(), 'b-o')\n",
        "plt.plot(z_vals[0].cpu().numpy(), y_vals[0].cpu().numpy(), 'r-o')\n",
        "plt.ylim([-1, 2])\n",
        "plt.title('Stratified Sampling (blue) with Perturbation (red)')\n",
        "ax = plt.gca()\n",
        "ax.axes.yaxis.set_visible(False)\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc0cS74xFb-G"
      },
      "source": [
        "## Positional Encoder\n",
        "\n",
        "Much like Transformers, NeRFs make use of positional encoders. In this case, it's to map the inputs to a higher frequency space to compensate for the bias that neural networks have for learning lower-frequency functions.\n",
        "\n",
        "Here we build a simple `torch.nn.Module` of our positional encoder. The same encoder implementation can be applied to both input samples and view directions. However, we choose different parameters for these inputs. We use the default settings from the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrbs7YoMHAbF"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    r\"\"\"\n",
        "    Sine-cosine positional encoder for input points.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_input: int,\n",
        "        n_freqs: int,\n",
        "        log_space: bool = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_input = d_input\n",
        "        self.n_freqs = n_freqs\n",
        "        self.log_space = log_space\n",
        "        self.d_output = d_input * (1 + 2 * self.n_freqs)\n",
        "        self.embed_fns = [lambda x: x]\n",
        "\n",
        "        # Define frequencies in either linear or log scale\n",
        "        if self.log_space:\n",
        "            freq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
        "        else:\n",
        "            freq_bands = torch.linspace(\n",
        "                2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
        "\n",
        "        # Alternate sin and cos\n",
        "        for freq in freq_bands:\n",
        "            self.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
        "            self.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x\n",
        "    ) -> torch.Tensor:\n",
        "        r\"\"\"\n",
        "        Apply positional encoding to input.\n",
        "        \"\"\"\n",
        "        return torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = PositionalEncoder(3, 10)\n",
        "viewdirs_encoder = PositionalEncoder(3, 4)\n",
        "# Grab flattened points and view directions \n",
        "pts_flattened = pts.reshape(-1, 3)\n",
        "viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "flattened_viewdirs = viewdirs[:, None, ...].expand(pts.shape).reshape((-1, 3))\n",
        "\n",
        "# Encode inputs\n",
        "encoded_points = encoder(pts_flattened)\n",
        "encoded_viewdirs = viewdirs_encoder(flattened_viewdirs)\n",
        "\n",
        "print('Encoded Points')\n",
        "print(encoded_points.shape)\n",
        "print(torch.min(encoded_points), torch.max(encoded_points), torch.mean(encoded_points))\n",
        "print('')\n",
        "\n",
        "print(encoded_viewdirs.shape)\n",
        "print('Encoded Viewdirs')\n",
        "print(torch.min(encoded_viewdirs), torch.max(encoded_viewdirs), torch.mean(encoded_viewdirs))\n",
        "print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94jKMMz7gb7Y"
      },
      "source": [
        "## NeRF Model\n",
        "\n",
        "Here we define the NeRF model, which consists primarily of a `ModuleList` of `Linear` layers, separated by non-linear activation functions and the occasional residual connection. This model features an optional input for view directions, which will alter the model architecture if provided at instantiation. This implementation is based on Section 3 of the original \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\" paper and uses the same defaults."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvIDtf3TgaOA"
      },
      "outputs": [],
      "source": [
        "class NeRF(nn.Module):\n",
        "  r\"\"\"\n",
        "  Neural radiance fields module.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    d_input: int = 3,\n",
        "    n_layers: int = 8,\n",
        "    d_filter: int = 256,\n",
        "    skip: Tuple[int] = (4,),\n",
        "    d_viewdirs: Optional[int] = None\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.d_input = d_input\n",
        "    self.skip = skip\n",
        "    self.act = nn.functional.relu\n",
        "    self.d_viewdirs = d_viewdirs\n",
        "\n",
        "    # Create model layers\n",
        "    self.layers = nn.ModuleList(\n",
        "      [nn.Linear(self.d_input, d_filter)] +\n",
        "      [nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
        "       else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
        "    )\n",
        "\n",
        "    # Bottleneck layers\n",
        "    if self.d_viewdirs is not None:\n",
        "      # If using viewdirs, split alpha and RGB\n",
        "      self.alpha_out = nn.Linear(d_filter, 1)\n",
        "      self.rgb_filters = nn.Linear(d_filter, d_filter)\n",
        "      self.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
        "      self.output = nn.Linear(d_filter // 2, 3)\n",
        "    else:\n",
        "      # If no viewdirs, use simpler output\n",
        "      self.output = nn.Linear(d_filter, 4)\n",
        "\n",
        "  def forward(\n",
        "    self,\n",
        "    x: torch.Tensor,\n",
        "    viewdirs: Optional[torch.Tensor] = None\n",
        "  ) -> torch.Tensor:\n",
        "    r\"\"\"\n",
        "    Forward pass with optional view direction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Cannot use viewdirs if instantiated with d_viewdirs = None\n",
        "    if self.d_viewdirs is None and viewdirs is not None:\n",
        "      raise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
        "\n",
        "    # Apply forward pass up to bottleneck\n",
        "    x_input = x\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      x = self.act(layer(x))\n",
        "      if i in self.skip:\n",
        "        x = torch.cat([x, x_input], dim=-1)\n",
        "\n",
        "    # Apply bottleneck\n",
        "    if self.d_viewdirs is not None:\n",
        "      # Split alpha from network output\n",
        "      alpha = self.alpha_out(x)\n",
        "\n",
        "      # Pass through bottleneck to get RGB\n",
        "      x = self.rgb_filters(x)\n",
        "      x = torch.concat([x, viewdirs], dim=-1)\n",
        "      x = self.act(self.branch(x))\n",
        "      x = self.output(x)\n",
        "\n",
        "      # Concatenate alphas to output\n",
        "      x = torch.concat([x, alpha], dim=-1)\n",
        "    else:\n",
        "      # Simple output\n",
        "      x = self.output(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xckd4RuFitJ"
      },
      "source": [
        "## Volume Rendering\n",
        "\n",
        "From the raw NeRF outputs, we still need to convert these into an image. This is where we apply the volume integration described in Equations 1-3 in Section 4 of the paper. Essentially, we take the weighted sum of all samples along the ray of each pixel to get the estimated color value at that pixel. Each RGB sample is weighted by its alpha value. Higher alpha values indicate higher likelihood that the sampled area is opaque, therefore points further along the ray are likelier to be occluded. The cumulative product ensures that those further points are dampened."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8Dvz6DszOUT"
      },
      "outputs": [],
      "source": [
        "def cumprod_exclusive(\n",
        "  tensor: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "  r\"\"\"\n",
        "  (Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
        "\n",
        "  Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
        "\n",
        "  Args:\n",
        "  tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
        "    is to be computed.\n",
        "  Returns:\n",
        "  cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
        "    tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
        "  \"\"\"\n",
        "\n",
        "  # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
        "  # tf.math.cumprod([a, b, c])  # [a, a * b, a * b * c]\n",
        "  cumprod = torch.cumprod(tensor, -1)\n",
        "  # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
        "  cumprod = torch.roll(cumprod, 1, -1)\n",
        "  # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
        "  # tf.math.cumprod([a, b, c], exclusive=True)  # [1, a, a * b]\n",
        "  cumprod[..., 0] = 1.\n",
        "\n",
        "  return cumprod\n",
        "\n",
        "def raw2outputs(\n",
        "  raw: torch.Tensor,\n",
        "  z_vals: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  raw_noise_std: float = 0.0,\n",
        "  white_bkgd: bool = False\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Convert the raw NeRF output into RGB and other maps.\n",
        "  \"\"\"\n",
        "\n",
        "  # Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
        "  dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
        "  dists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
        "\n",
        "  # Multiply each distance by the norm of its corresponding direction ray\n",
        "  # to convert to real world distance (accounts for non-unit directions).\n",
        "  dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
        "\n",
        "  # Add noise to model's predictions for density. Can be used to\n",
        "  # regularize network during training (prevents floater artifacts).\n",
        "  noise = 0.\n",
        "  if raw_noise_std > 0.:\n",
        "    noise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
        "\n",
        "  # Predict density of each sample along each ray. Higher values imply\n",
        "  # higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
        "  alpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists)\n",
        "\n",
        "  # Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
        "  # The higher the alpha, the lower subsequent weights are driven.\n",
        "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
        "\n",
        "  # Compute weighted RGB map.\n",
        "  rgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
        "  rgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
        "\n",
        "  # Estimated depth map is predicted distance.\n",
        "  depth_map = torch.sum(weights * z_vals, dim=-1)\n",
        "\n",
        "  # Disparity map is inverse depth.\n",
        "  # disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
        "  #                           depth_map / torch.sum(weights, -1))\n",
        "\n",
        "  # Sum of weights along each ray. In [0, 1] up to numerical error.\n",
        "  acc_map = torch.sum(weights, dim=-1)\n",
        "\n",
        "  # To composite onto a white background, use the accumulated alpha map.\n",
        "  if white_bkgd:\n",
        "    rgb_map = rgb_map + (1. - acc_map[..., None])\n",
        "\n",
        "  return rgb_map, depth_map, acc_map, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySqZmYMRFoF1"
      },
      "source": [
        "## Hierarchical Volume Sampling\n",
        "\n",
        "The 3D space is in fact very sparse with occlusions and so most points don't contribute much to the rendered image. It is therefore more beneficial to oversample regions with a high likelihood of contributing to the integral. Here we apply learned, normalized weights to the first set of samples to create a PDF across the ray, then apply inverse transform sampling to this PDF to gather a second set of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK2x2JEFFnxA"
      },
      "outputs": [],
      "source": [
        "def sample_pdf(\n",
        "    bins: torch.Tensor,\n",
        "    weights: torch.Tensor,\n",
        "    n_samples: int,\n",
        "    perturb: bool = False\n",
        ") -> torch.Tensor:\n",
        "    r\"\"\"\n",
        "    Apply inverse transform sampling to a weighted set of points.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize weights to get PDF.\n",
        "    pdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -\n",
        "                                       1, keepdims=True)  # [n_rays, weights.shape[-1]]\n",
        "\n",
        "    # Convert PDF to CDF.\n",
        "    cdf = torch.cumsum(pdf, dim=-1)  # [n_rays, weights.shape[-1]]\n",
        "    # [n_rays, weights.shape[-1] + 1]\n",
        "    cdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)\n",
        "\n",
        "    # Take sample positions to grab from CDF. Linear when perturb == 0.\n",
        "    if not perturb:\n",
        "        u = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
        "        u = u.expand(list(cdf.shape[:-1]) + [n_samples])  # [n_rays, n_samples]\n",
        "    else:\n",
        "        # [n_rays, n_samples]\n",
        "        u = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device)\n",
        "\n",
        "    # Find indices along CDF where values in u would be placed.\n",
        "    u = u.contiguous()  # Returns contiguous tensor with same values.\n",
        "    inds = torch.searchsorted(cdf, u, right=True)  # [n_rays, n_samples]\n",
        "\n",
        "    # Clamp indices that are out of bounds.\n",
        "    below = torch.clamp(inds - 1, min=0)\n",
        "    above = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
        "    inds_g = torch.stack([below, above], dim=-1)  # [n_rays, n_samples, 2]\n",
        "\n",
        "    # Sample from cdf and the corresponding bin centers.\n",
        "    matched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]] # [n_rays, n_samples, cdf.shape[-1]]\n",
        "    # [n_rays, n_samples, cdf.shape[-1]]\n",
        "    cdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
        "                         index=inds_g)\n",
        "    # [n_rays, n_samples, cdf.shape[-1]]\n",
        "    bins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
        "                          index=inds_g)\n",
        "\n",
        "    # Convert samples to ray length.\n",
        "    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
        "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
        "    t = (u - cdf_g[..., 0]) / denom\n",
        "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
        "\n",
        "    return samples  # [n_rays, n_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Example CDF tensor (simplified for illustration)\n",
        "cdf = torch.tensor([[0.1, 0.35, 0.6, 1.0], [0.2, 0.4, 0.7, 1.0]])\n",
        "\n",
        "# Example sample positions\n",
        "u = torch.tensor([[0.2, 0.8], [0.4, 0.6]])\n",
        "\n",
        "# Calculate inds using searchsorted\n",
        "inds = torch.searchsorted(cdf, u, right=True)\n",
        "\n",
        "print(\"CDF:\")\n",
        "print(cdf)\n",
        "print(\"\\nSample Positions (u):\")\n",
        "print(u)\n",
        "print(\"\\nIndices (inds):\")\n",
        "print(inds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU4qRGMhNNHu"
      },
      "outputs": [],
      "source": [
        "def sample_hierarchical(\n",
        "  rays_o: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  z_vals: torch.Tensor,\n",
        "  weights: torch.Tensor,\n",
        "  n_samples: int,\n",
        "  perturb: bool = False\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Apply hierarchical sampling to the rays.\n",
        "  \"\"\"\n",
        "\n",
        "  # Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
        "  z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
        "  new_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
        "                          perturb=perturb)\n",
        "  new_z_samples = new_z_samples.detach()\n",
        "\n",
        "  # Resample points from ray based on PDF.\n",
        "  z_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
        "  pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
        "  return pts, z_vals_combined, new_z_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFUaajNpNNgJ"
      },
      "source": [
        "## Full Forward Pass\n",
        "\n",
        "Here is where we put everything together to compute a single forward pass through our model.\n",
        "\n",
        "Due to potential memory issues, the forward pass is computed in \"chunks,\" which are then aggregated across a single batch. The gradient propagation is done after the whole batch is processed, hence the distinction between \"chunks\" and \"batches.\" Chunking is especially important for the Google Colab environment, which provides more modest resources than those cited in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9SbAqC6Ki9H"
      },
      "outputs": [],
      "source": [
        "def get_chunks(\n",
        "  inputs: torch.Tensor,\n",
        "  chunksize: int = 2**15\n",
        ") -> List[torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Divide an input into chunks.\n",
        "  \"\"\"\n",
        "  return [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
        "\n",
        "def prepare_chunks(\n",
        "  points: torch.Tensor,\n",
        "  encoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
        "  chunksize: int = 2**15\n",
        ") -> List[torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Encode and chunkify points to prepare for NeRF model.\n",
        "  \"\"\"\n",
        "  points = points.reshape((-1, 3))\n",
        "  points = encoding_function(points)\n",
        "  points = get_chunks(points, chunksize=chunksize)\n",
        "  return points\n",
        "\n",
        "def prepare_viewdirs_chunks(\n",
        "  points: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  encoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
        "  chunksize: int = 2**15\n",
        ") -> List[torch.Tensor]:\n",
        "  r\"\"\"\n",
        "  Encode and chunkify viewdirs to prepare for NeRF model.\n",
        "  \"\"\"\n",
        "  # Prepare the viewdirs\n",
        "  viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "  viewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
        "  viewdirs = encoding_function(viewdirs)\n",
        "  viewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
        "  return viewdirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY2Dt8lgWhKO"
      },
      "outputs": [],
      "source": [
        "def nerf_forward(\n",
        "  rays_o: torch.Tensor,\n",
        "  rays_d: torch.Tensor,\n",
        "  near: float,\n",
        "  far: float,\n",
        "  encoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
        "  coarse_model: nn.Module,\n",
        "  kwargs_sample_stratified: dict = None,\n",
        "  n_samples_hierarchical: int = 0,\n",
        "  kwargs_sample_hierarchical: dict = None,\n",
        "  fine_model = None,\n",
        "  viewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
        "  chunksize: int = 2**15\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
        "  r\"\"\"\n",
        "  Compute forward pass through model(s).\n",
        "  \"\"\"\n",
        "\n",
        "  # Set no kwargs if none are given.\n",
        "  if kwargs_sample_stratified is None:\n",
        "    kwargs_sample_stratified = {}\n",
        "  if kwargs_sample_hierarchical is None:\n",
        "    kwargs_sample_hierarchical = {}\n",
        "\n",
        "  # Sample query points along each ray.\n",
        "  query_points, z_vals = sample_stratified(\n",
        "      rays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
        "\n",
        "  # Prepare batches.\n",
        "  batches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
        "  if viewdirs_encoding_fn is not None:\n",
        "    batches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
        "                                               viewdirs_encoding_fn,\n",
        "                                               chunksize=chunksize)\n",
        "  else:\n",
        "    batches_viewdirs = [None] * len(batches)\n",
        "\n",
        "  # Coarse model pass.\n",
        "  # Split the encoded points into \"chunks\", run the model on all chunks, and\n",
        "  # concatenate the results (to avoid out-of-memory issues).\n",
        "  predictions = []\n",
        "  for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
        "    predictions.append(coarse_model(batch, viewdirs=batch_viewdirs))\n",
        "  raw = torch.cat(predictions, dim=0)\n",
        "  raw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
        "\n",
        "  # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
        "  rgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
        "  # rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
        "  outputs = {\n",
        "      'z_vals_stratified': z_vals\n",
        "  }\n",
        "\n",
        "  # Fine model pass.\n",
        "  if n_samples_hierarchical > 0:\n",
        "    # Save previous outputs to return.\n",
        "    rgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
        "\n",
        "    # Apply hierarchical sampling for fine query points.\n",
        "    query_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
        "      rays_o, rays_d, z_vals, weights, n_samples_hierarchical,\n",
        "      **kwargs_sample_hierarchical)\n",
        "\n",
        "    # Prepare inputs as before.\n",
        "    batches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
        "    if viewdirs_encoding_fn is not None:\n",
        "      batches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
        "                                                 viewdirs_encoding_fn,\n",
        "                                                 chunksize=chunksize)\n",
        "    else:\n",
        "      batches_viewdirs = [None] * len(batches)\n",
        "\n",
        "    # Forward pass new samples through fine model.\n",
        "    fine_model = fine_model if fine_model is not None else coarse_model\n",
        "    predictions = []\n",
        "    for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
        "      predictions.append(fine_model(batch, viewdirs=batch_viewdirs))\n",
        "    raw = torch.cat(predictions, dim=0)\n",
        "    raw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
        "\n",
        "    # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
        "    rgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
        "\n",
        "    # Store outputs.\n",
        "    outputs['z_vals_hierarchical'] = z_hierarch\n",
        "    outputs['rgb_map_0'] = rgb_map_0\n",
        "    outputs['depth_map_0'] = depth_map_0\n",
        "    outputs['acc_map_0'] = acc_map_0\n",
        "\n",
        "  # Store outputs.\n",
        "  outputs['rgb_map'] = rgb_map\n",
        "  outputs['depth_map'] = depth_map\n",
        "  outputs['acc_map'] = acc_map\n",
        "  outputs['weights'] = weights\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtpbzoRJYsK-"
      },
      "source": [
        "# Train\n",
        "\n",
        "At long last, we have (almost) everything we need to train the model. Now we will do some setup for a simple training procedure, creating hyperparameters and helper functions, then train our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBXWfThMYtkR"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "All hyperparameters for training are set here. Defaults were taken from the original, unless computational constraints prohibit them. In this case, we apply sensible defaults that are well within the resources provided by Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JovhcSy1NIhr"
      },
      "outputs": [],
      "source": [
        "# Encoders\n",
        "d_input = 3           # Number of input dimensions\n",
        "n_freqs = 10          # Number of encoding functions for samples\n",
        "log_space = True      # If set, frequencies scale in log space\n",
        "use_viewdirs = True   # If set, use view direction as input\n",
        "n_freqs_views = 4     # Number of encoding functions for views\n",
        "\n",
        "# Stratified sampling\n",
        "n_samples = 64         # Number of spatial samples per ray\n",
        "perturb = True         # If set, applies noise to sample positions\n",
        "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
        "\n",
        "# Model\n",
        "d_filter = 128          # Dimensions of linear layer filters\n",
        "n_layers = 2            # Number of layers in network bottleneck\n",
        "skip = []               # Layers at which to apply input residual\n",
        "use_fine_model = True   # If set, creates a fine model\n",
        "d_filter_fine = 128     # Dimensions of linear layer filters of fine network\n",
        "n_layers_fine = 6       # Number of layers in fine network bottleneck\n",
        "\n",
        "# Hierarchical sampling\n",
        "n_samples_hierarchical = 64   # Number of samples per ray\n",
        "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
        "\n",
        "# Optimizer\n",
        "lr = 5e-4  # Learning rate\n",
        "\n",
        "# Training\n",
        "n_iters = 10000\n",
        "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
        "one_image_per_step = True   # One image per gradient step (disables batching)\n",
        "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
        "center_crop = True          # Crop the center of image (one_image_per_)\n",
        "center_crop_iters = 50      # Stop cropping center after this many epochs\n",
        "display_rate = 25          # Display test output every X epochs\n",
        "\n",
        "# Early Stopping\n",
        "warmup_iters = 100          # Number of iterations during warmup phase\n",
        "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
        "n_restarts = 10             # Number of times to restart if training stalls\n",
        "\n",
        "# We bundle the kwargs for various functions to pass all at once.\n",
        "kwargs_sample_stratified = {\n",
        "    'n_samples': n_samples,\n",
        "    'perturb': perturb,\n",
        "    'inverse_depth': inverse_depth\n",
        "}\n",
        "kwargs_sample_hierarchical = {\n",
        "    'perturb': perturb\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODAQKoAUY0KJ"
      },
      "source": [
        "## Training Classes and Functions\n",
        "\n",
        "Here we create some helper functions for training. NeRF can be prone to local minima, in which training will quickly stall and produce blank outputs. `EarlyStopping` is used to restart the training when learning stalls, if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeIrS4P54piy"
      },
      "outputs": [],
      "source": [
        "def plot_samples(\n",
        "  z_vals: torch.Tensor,\n",
        "  z_hierarch: Optional[torch.Tensor] = None,\n",
        "  ax: Optional[np.ndarray] = None):\n",
        "  r\"\"\"\n",
        "  Plot stratified and (optional) hierarchical samples.\n",
        "  \"\"\"\n",
        "  y_vals = 1 + np.zeros_like(z_vals)\n",
        "\n",
        "  if ax is None:\n",
        "    ax = plt.subplot()\n",
        "  ax.plot(z_vals, y_vals, 'b-o')\n",
        "  if z_hierarch is not None:\n",
        "    y_hierarch = np.zeros_like(z_hierarch)\n",
        "    ax.plot(z_hierarch, y_hierarch, 'r-o')\n",
        "  ax.set_ylim([-1, 2])\n",
        "  ax.set_title('Stratified  Samples (blue) and Hierarchical Samples (red)')\n",
        "  ax.axes.yaxis.set_visible(False)\n",
        "  ax.grid(True)\n",
        "  return ax\n",
        "\n",
        "def crop_center(\n",
        "  img: torch.Tensor,\n",
        "  frac: float = 0.5\n",
        ") -> torch.Tensor:\n",
        "  r\"\"\"\n",
        "  Crop center square from image.\n",
        "  \"\"\"\n",
        "  h_offset = round(img.shape[0] * (frac / 2))\n",
        "  w_offset = round(img.shape[1] * (frac / 2))\n",
        "  return img[h_offset:-h_offset, w_offset:-w_offset]\n",
        "\n",
        "class EarlyStopping:\n",
        "  r\"\"\"\n",
        "  Early stopping helper based on fitness criterion.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    patience: int = 30,\n",
        "    margin: float = 1e-4\n",
        "  ):\n",
        "    self.best_fitness = 0.0  # In our case PSNR\n",
        "    self.best_iter = 0\n",
        "    self.margin = margin\n",
        "    self.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\n",
        "\n",
        "  def __call__(\n",
        "    self,\n",
        "    iter: int,\n",
        "    fitness: float\n",
        "  ):\n",
        "    r\"\"\"\n",
        "    Check if criterion for stopping is met.\n",
        "    \"\"\"\n",
        "    if (fitness - self.best_fitness) > self.margin:\n",
        "      self.best_iter = iter\n",
        "      self.best_fitness = fitness\n",
        "    delta = iter - self.best_iter\n",
        "    stop = delta >= self.patience  # stop training if patience exceeded\n",
        "    return stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXTZ79RxYSXA"
      },
      "outputs": [],
      "source": [
        "def init_models():\n",
        "  r\"\"\"\n",
        "  Initialize models, encoders, and optimizer for NeRF training.\n",
        "  \"\"\"\n",
        "  # Encoders\n",
        "  encoder = PositionalEncoder(d_input, n_freqs, log_space=log_space)\n",
        "  encode = lambda x: encoder(x)\n",
        "\n",
        "  # View direction encoders\n",
        "  if use_viewdirs:\n",
        "    encoder_viewdirs = PositionalEncoder(d_input, n_freqs_views,\n",
        "                                        log_space=log_space)\n",
        "    encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
        "    d_viewdirs = encoder_viewdirs.d_output\n",
        "  else:\n",
        "    encode_viewdirs = None\n",
        "    d_viewdirs = None\n",
        "\n",
        "  # Models\n",
        "  model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
        "              d_viewdirs=d_viewdirs)\n",
        "  model.to(device)\n",
        "  model_params = list(model.parameters())\n",
        "  if use_fine_model:\n",
        "    fine_model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
        "                      d_viewdirs=d_viewdirs)\n",
        "    fine_model.to(device)\n",
        "    model_params = model_params + list(fine_model.parameters())\n",
        "  else:\n",
        "    fine_model = None\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.Adam(model_params, lr=lr)\n",
        "\n",
        "  # Early Stopping\n",
        "  warmup_stopper = EarlyStopping(patience=50)\n",
        "\n",
        "  return model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH6R8jD_Ywp6"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Here we start training our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r87VAJ0E7aKQ"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  r\"\"\"\n",
        "  Launch training session for NeRF.\n",
        "  \"\"\"\n",
        "  # Shuffle rays across all images.\n",
        "  if not one_image_per_step:\n",
        "    height, width = images.shape[1:3]\n",
        "    all_rays = torch.stack([torch.stack(get_rays(height, width, focal, p), 0)\n",
        "                        for p in poses[:n_training]], 0)\n",
        "    rays_rgb = torch.cat([all_rays, images[:, None]], 1)\n",
        "    rays_rgb = torch.permute(rays_rgb, [0, 2, 3, 1, 4])\n",
        "    rays_rgb = rays_rgb.reshape([-1, 3, 3])\n",
        "    rays_rgb = rays_rgb.type(torch.float32)\n",
        "    rays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n",
        "    i_batch = 0\n",
        "\n",
        "  train_psnrs = []\n",
        "  val_psnrs = []\n",
        "  iternums = []\n",
        "  for i in trange(n_iters):\n",
        "    model.train()\n",
        "\n",
        "    if one_image_per_step:\n",
        "      # Randomly pick an image as the target.\n",
        "      target_img_idx = np.random.randint(images.shape[0])\n",
        "      target_img = images[target_img_idx].to(device)\n",
        "      if center_crop and i < center_crop_iters:\n",
        "        target_img = crop_center(target_img)\n",
        "      height, width = target_img.shape[:2]\n",
        "      target_pose = poses[target_img_idx].to(device)\n",
        "      rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
        "      rays_o = rays_o.reshape([-1, 3])\n",
        "      rays_d = rays_d.reshape([-1, 3])\n",
        "    else:\n",
        "      # Random over all images.\n",
        "      batch = rays_rgb[i_batch:i_batch + batch_size]\n",
        "      batch = torch.transpose(batch, 0, 1)\n",
        "      rays_o, rays_d, target_img = batch\n",
        "      height, width = target_img.shape[:2]\n",
        "      i_batch += batch_size\n",
        "      # Shuffle after one epoch\n",
        "      if i_batch >= rays_rgb.shape[0]:\n",
        "          rays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n",
        "          i_batch = 0\n",
        "    target_img = target_img.reshape([-1, 3])\n",
        "\n",
        "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
        "    outputs = nerf_forward(rays_o, rays_d,\n",
        "                           near, far, encode, model,\n",
        "                           kwargs_sample_stratified=kwargs_sample_stratified,\n",
        "                           n_samples_hierarchical=n_samples_hierarchical,\n",
        "                           kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
        "                           fine_model=fine_model,\n",
        "                           viewdirs_encoding_fn=encode_viewdirs,\n",
        "                           chunksize=chunksize)\n",
        "\n",
        "    # Check for any numerical issues.\n",
        "    for k, v in outputs.items():\n",
        "      if torch.isnan(v).any():\n",
        "        print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
        "      if torch.isinf(v).any():\n",
        "        print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
        "\n",
        "    # Backprop!\n",
        "    rgb_predicted = outputs['rgb_map']\n",
        "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    psnr = -10. * torch.log10(loss)\n",
        "    train_psnrs.append(psnr.item())\n",
        "\n",
        "    # Evaluate testimg at given display rate.\n",
        "    if i % display_rate == 0:\n",
        "      model.eval()\n",
        "      height, width = testimg.shape[:2]\n",
        "      rays_o, rays_d = get_rays(height, width, focal, testpose)\n",
        "      rays_o = rays_o.reshape([-1, 3])\n",
        "      rays_d = rays_d.reshape([-1, 3])\n",
        "      outputs = nerf_forward(rays_o, rays_d,\n",
        "                             near, far, encode, model,\n",
        "                             kwargs_sample_stratified=kwargs_sample_stratified,\n",
        "                             n_samples_hierarchical=n_samples_hierarchical,\n",
        "                             kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
        "                             fine_model=fine_model,\n",
        "                             viewdirs_encoding_fn=encode_viewdirs,\n",
        "                             chunksize=chunksize)\n",
        "\n",
        "      rgb_predicted = outputs['rgb_map']\n",
        "      loss = torch.nn.functional.mse_loss(rgb_predicted, testimg.reshape(-1, 3))\n",
        "      print(\"Loss:\", loss.item())\n",
        "      val_psnr = -10. * torch.log10(loss)\n",
        "      val_psnrs.append(val_psnr.item())\n",
        "      iternums.append(i)\n",
        "\n",
        "      # Plot example outputs\n",
        "      fig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
        "      ax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
        "      ax[0].set_title(f'Iteration: {i}')\n",
        "      ax[1].imshow(testimg.detach().cpu().numpy())\n",
        "      ax[1].set_title(f'Target')\n",
        "      ax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
        "      ax[2].plot(iternums, val_psnrs, 'b')\n",
        "      ax[2].set_title('PSNR (train=red, val=blue')\n",
        "      z_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
        "      z_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
        "      if 'z_vals_hierarchical' in outputs:\n",
        "        z_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
        "        z_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
        "      else:\n",
        "        z_sample_hierarch = None\n",
        "      _ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
        "      ax[3].margins(0)\n",
        "      plt.show()\n",
        "\n",
        "    # Check PSNR for issues and stop if any are found.\n",
        "    if i == warmup_iters - 1:\n",
        "      if val_psnr < warmup_min_fitness:\n",
        "        print(f'Val PSNR {val_psnr} below warmup_min_fitness {warmup_min_fitness}. Stopping...')\n",
        "        return False, train_psnrs, val_psnrs\n",
        "    elif i < warmup_iters:\n",
        "      if warmup_stopper is not None and warmup_stopper(i, psnr):\n",
        "        print(f'Train PSNR flatlined at {psnr} for {warmup_stopper.patience} iters. Stopping...')\n",
        "        return False, train_psnrs, val_psnrs\n",
        "\n",
        "  return True, train_psnrs, val_psnrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "MnHIjU4IYfJp",
        "outputId": "bc62e7b8-ea20-4641-8654-740fdf02cfdc"
      },
      "outputs": [],
      "source": [
        "# Run training session(s)\n",
        "for _ in range(n_restarts):\n",
        "  model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper = init_models()\n",
        "  success, train_psnrs, val_psnrs = train()\n",
        "  if success and val_psnrs[-1] >= warmup_min_fitness:\n",
        "    print('Training successful!')\n",
        "    break\n",
        "\n",
        "print('')\n",
        "print(f'Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb3TZd06zFP0"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'nerf.pt')\n",
        "torch.save(fine_model.state_dict(), 'nerf-fine.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
